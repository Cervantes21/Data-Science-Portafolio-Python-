# -*- coding: utf-8 -*-
"""Caso practico de  Cyclistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SEe65S7OUjUv7t0wGcIOiV7Q4ieEU-RC

## Instrucción clara de la tarea empresarial:
El objetivo es identificar las diferencias en el uso de las bicicletas de Cyclistic entre los miembros anuales y los ciclistas ocasionales. El análisis de los datos históricos de viajes en bicicleta de Cyclistic permitirá determinar patrones y tendencias de uso que puedan ayudar a diseñar una estrategia de marketing efectiva para convertir a ciclistas ocasionales en miembros anuales.
"""

!pip install wget sklearn Tensorflow  # Instalamos la librería wget, Tensorflow y Sklearn

# Librerías para organizar los datos:
import requests
import wget
import zipfile

# Manipulación de los datos:
import numpy as np
import pandas as pd

# Visualización de los datos:
import seaborn as sns
import matplotlib.pyplot as plt

# Machine Learning:
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
import tensorflow as tf

"""## **Organización de los datos**

Primero crearemos una función para descargar nuestros datos de cada mes y así guardarlos en una lista, para que posteriormente los podamos guardar en variables, una vez que tengamos nuestras variables las uniremos a un solo conjunto de datos para poder realizar el **Análisis exploratorio de datos (EDA)**
"""

# Creamos una función para descargar nuestros datos de cada mes:
def download_file(url, filename):
    response = requests.get(url)
    with open(filename, 'wb') as f:
        f.write(response.content)

# '''
    #   Una lista con las URLs de los archivos de cada mes 
    #   para usarla en nuestro bucle para descargarlos y guardarlos con nombres descriptivos:
    # '''
urls = ['https://divvy-tripdata.s3.amazonaws.com/202201-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202202-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202203-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202204-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202205-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202206-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202207-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202208-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202209-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202210-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202211-divvy-tripdata.zip',
        'https://divvy-tripdata.s3.amazonaws.com/202212-divvy-tripdata.zip']

for url in urls:
  filename = url.split('/')[-1]
  download_file(url, filename)

# Extraer el archivo CSV del archivo ZIP
with zipfile.ZipFile('/content/202201-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]  # obtener el nombre del archivo CSV
    zip_ref.extract(csv_file)

# Leer el archivo CSV extraído en un DataFrame de Pandas
ene_22 = pd.read_csv(csv_file)


# Repetir para los demás meses
with zipfile.ZipFile('/content/202202-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
feb_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202203-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
mar_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202204-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
abr_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202205-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
may_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202206-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
jun_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202207-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
jul_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202208-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
ago_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202209-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
sep_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202210-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
oct_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202211-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
nov_22 = pd.read_csv(csv_file)

with zipfile.ZipFile('/content/202212-divvy-tripdata.zip', 'r') as zip_ref:
    csv_file = zip_ref.namelist()[0]
    zip_ref.extract(csv_file)
dic_22 = pd.read_csv(csv_file)

# Visualizamos nuestro primer conjunto para verificar:

ene_22

# También el último conjunto que en esté caso corresponde a Diciembre 2022
dic_22

data = pd.concat([ene_22, feb_22, mar_22, abr_22, may_22, jun_22, jul_22, ago_22, sep_22, oct_22, nov_22, dic_22])
print(f'Ahora nuestro conjunto de datos contiene {data.shape[0]} filas y {data.shape[1]} columnas')

"""## **Exploración de los datos**"""

# Comenzamos con la información que nos proporciona nuestro Dataset
data.info()

# Exploramos los datos nulos:
data.isnull().sum()

print('Tipos de biciletas:', data['rideable_type'].unique())
print('Tipos de usuarios:', data['member_casual'].unique())

# Eliminar filas duplicadas
data.drop_duplicates(inplace=True)
print(f'Ahora nuestro conjunto de datos quedo en: {data.shape[0]} filas y {data.shape[1]} columnas')

"""### Imputación de los datos:
la mediana es una buena opción para imputar los valores faltantes en este caso. Ya que corresponden a los puntos de las estaciones.
"""

# Observamos nuestros datos nulos:
print(data[['end_lat', 'end_lng']].isnull().sum())

data['end_lat'].fillna(value=data['end_lat'].median(), inplace=True)
data['end_lng'].fillna(value=data['end_lng'].median(), inplace=True)

data['end_lat']

data['end_lng']

"""## **Análisis exploratorio de datos**

"""

# Convertir "started_at" y "ended_at" a objetos datetime
data['started_at'] = pd.to_datetime(data['started_at'])
data['ended_at'] = pd.to_datetime(data['ended_at'])

# Calcular la duración del viaje en minutos
data['duration_min'] = (data['ended_at'] - data['started_at']).dt.seconds/60

# Agrupar los datos por "member_casual" y calcular estadísticas descriptivas de la duración del viaje
grouped_data = data.groupby('member_casual')['duration_min'].describe()
print(grouped_data)

"""## Visualización de datos:

Otra variable que podríamos explorar es la distribución del tiempo de uso de las bicicletas según el tipo de usuario.

Podemos graficar el tiempo de uso en minutos con un histograma para cada tipo de usuario:
"""

sns.histplot(data=data, x='duration_min', hue='member_casual', bins=50, kde=True)
plt.title('Distribución de duraciones de viajes por tipo de usuario')
plt.xlabel('Duración del viaje (minutos)')
plt.ylabel('Frecuencia')
plt.show()

"""Podemos mejorar la visualización dividiendo la duración del viaje en intervalos y contando cuántos viajes hay en cada intervalo para cada tipo de usuario. Luego podemos graficar esta información en un histograma apilado, donde cada color representa un tipo de usuario. Esto nos permitirá visualizar la distribución de los viajes por duración para cada tipo de usuario de manera más clara y significativa.


En este caso, estamos dividiendo la duración del viaje en intervalos de 5 minutos y generando un histograma apilado para cada tipo de usuario. También estamos agregando etiquetas a los ejes y un título al gráfico para hacerlo más claro y comprensible.
"""

sns.histplot(data=data, x='duration_min', hue='member_casual', bins=range(0, 120, 5), multiple="stack")
plt.xlabel('Duración del viaje (min)')
plt.ylabel('Cantidad de viajes')
plt.title('Distribución de la duración del viaje por tipo de usuario')
plt.show()

# Convertir "started_at" y "ended_at" a objetos datetime
data['started_at'] = pd.to_datetime(data['started_at'])
data['ended_at'] = pd.to_datetime(data['ended_at'])

# Crear columnas para día de la semana y hora del día
data['day_of_week'] = data['started_at'].dt.day_name()
data['hour'] = data['started_at'].dt.hour

# Agrupar los datos por día de la semana y hora del día y calcular la duración media de los viajes
grouped_data = data.groupby(['day_of_week', 'hour'])['duration_min'].mean().reset_index()

# Visualizar los datos con un heatmap

plt.figure(figsize=(12, 6))
sns.heatmap(grouped_data.pivot('day_of_week', 'hour', 'duration_min'), cmap="YlGnBu")
plt.title('Duración media de los viajes por día de la semana y hora del día')
plt.xlabel('Hora del día')
plt.ylabel('Día de la semana')
plt.show()

# Creamos un DataFrame con el número de viajes por mes y año
grouped = data.groupby([data['started_at'].dt.year.rename('year'), data['started_at'].dt.month.rename('month')])['ride_id'].count().reset_index()

# Convertimos los meses en nombres para mejorar la visualización
grouped['month'] = pd.to_datetime(grouped['month'], format='%m').dt.month_name()

# Creamos una figura de tamaño 10x6
fig, ax = plt.subplots(figsize=(10,6))

# Creamos un gráfico de barras agrupadas con los datos de cada año
sns.barplot(data=grouped, x='month', y='ride_id', hue='year')

# Cambiamos el tamaño de la fuente del título y los ejes
ax.set_title('Número de viajes por mes y año', fontsize=18, y=1.05)
ax.set_xlabel('Mes', fontsize=14)
ax.set_ylabel('Número de viajes', fontsize=14)
ax.tick_params(labelsize=12, rotation=45) # Rotamos las etiquetas de los meses
ax.set_xticklabels(ax.get_xticklabels(), fontsize=12) # Ajustamos el tamaño de las etiquetas de los meses
# Agregamos una leyenda para indicar los colores de cada año
ax.legend(title='Año', fontsize=12)

# Mostramos el gráfico
plt.show()

"""podemos filtrar nuestro conjunto de datos data para obtener únicamente las duraciones de los viajes de los usuarios casuales y miembros habituales:"""

# Convertir "started_at" y "ended_at" a objetos datetime
data['started_at'] = pd.to_datetime(data['started_at'])
data['ended_at'] = pd.to_datetime(data['ended_at'])

# Crear columnas para el día de la semana y la hora del día
data['day_of_week'] = data['started_at'].dt.day_name()
data['hour'] = data['started_at'].dt.hour

# Agrupar los datos por "day_of_week" y "hour" y calcular estadísticas descriptivas de la duración del viaje
grouped_data = data.groupby(['day_of_week', 'hour'])['duration_min'].describe()

# Creamos un diccionario para cambiar los nombres de los días de la semana
day_dict = {0:'Lunes', 1:'Martes', 2:'Miércoles', 3:'Jueves', 4:'Viernes', 5:'Sábado', 6:'Domingo'}

# Cambiamos los valores numéricos por los nombres de los días de la semana
grouped_data['day_of_week'] = grouped_data.index.get_level_values(0)
grouped_data['hour'] = grouped_data.index.get_level_values(1)
grouped_data['day_of_week'] = grouped_data['day_of_week'].map(day_dict)

# Creamos un gráfico de línea con el variable hour en el eje x, counts en el eje y y distintos colores para cada tipo de miembro
sns.set_style('darkgrid')
g = sns.relplot(data=grouped_data, x='hour', y='mean', hue='day_of_week', kind='line', aspect=1.5)

# Cambiamos el tamaño de la fuente del título y los ejes
g.fig.suptitle('Duración promedio de viajes por hora del día, por día de la semana', fontsize=18, y=1.05)
g.set_axis_labels('Hora del día', 'Duración promedio del viaje (min)')
g.ax.tick_params(labelsize=12)
g.ax.set_xticks(range(0, 24, 2))

# Mostramos el gráfico
plt.show()

"""## Algoritmos con Machine Learning

"""

# Dividir los datos en entrenamiento (80%) y prueba (20%)
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Verificar el tamaño de los conjuntos de datos
print("Tamaño del conjunto de entrenamiento:", len(train_data))
print("Tamaño del conjunto de prueba:", len(test_data))